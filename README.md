# Transformer

Transformer implementation based on the paper _Attention is All You Need_ (Vaswani et al., 2017).

## Overview
This repository contains a from-scratch implementation of the Transformer architecture using PyTorch. The Transformer model uses a mechanism that relies on self-attention to compute representations without using sequence-aligned recurrent or convolutional neural networks. The transformer follows the encoder-decoder architecture.

## References
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2017). Attention Is All You Need
